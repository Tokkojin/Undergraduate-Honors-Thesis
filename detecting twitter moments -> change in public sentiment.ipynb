{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Twitter moments that have significant impact on a public sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting the data\n",
    "\n",
    "We use [Twitterscraper](https://github.com/taspinar/twitterscraper) to collect tweets that mention our celebrity figures of note."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "\n",
    "import json\n",
    "\n",
    "import sys\n",
    "\n",
    "from twitterscraper import query_tweets\n",
    "\n",
    "def datetime_handler(x):\n",
    "    if isinstance(x, dt.datetime):\n",
    "        return x.isoformat()\n",
    "    raise TypeError('Unknown type')\n",
    "\n",
    "def collect_tweets(name, articleDate):\n",
    "    name = name.lower()\n",
    "\n",
    "    articleDate = datetime.strptime(articleDate, '%m/%d/%y')\n",
    "    beginDate = (articleDate - timedelta(days=90)).date()\n",
    "    endDate = (articleDate + timedelta(days=90)).date()\n",
    "\n",
    "    # Collect tweets with mentions in the form of \"FirstName LastName\"\n",
    "    tweets = query_tweets(name, limit=None, begindate=beginDate, enddate=endDate, poolsize=40, lang='en')\n",
    "    tweets_serialized_pt1 = [tweet.__dict__ for tweet in tweets]\n",
    "\n",
    "    # Collect tweets with mentions in the form of \"FirstNameLastName\"\n",
    "    no_space_name = name.replace(' ', '')\n",
    "\n",
    "    tweets = query.query_tweets(no_space_name, limit=None, begindate=beginDate, enddate=endDate, poolsize=40, lang='en')\n",
    "    tweets_serialized_pt2 = [tweet.__dict__ for tweet in tweets]\n",
    "\n",
    "    tweets_serialized = tweets_serialized_pt1 + tweets_serialized_pt2\n",
    "\n",
    "    with open(outfile_str, 'w') as outfile:\n",
    "        json.dump(tweets_serialized, outfile, default=datetime_handler)\n",
    "        print('tweets saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input(\"Name (FirstName LastName): \")\n",
    "articleDate = input(\"Article date (mm/dd/yy): \")\n",
    "\n",
    "print('Collecting tweets for ' + name)\n",
    "print('Article release ~ ' + articleDate + '\\n')\n",
    "\n",
    "collect_tweets(name, articleDate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.tokenize import treebank\n",
    "\n",
    "import sys\n",
    "\n",
    "from multiprocessing import cpu_count, Pool\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tokenizer = treebank.TreebankWordTokenizer()\n",
    "\n",
    "def get_lexicon_polarity(row):\n",
    "    # Possible improvements: Make entity-based; consider lexicon in context\n",
    "    polarity = 'NaN'\n",
    "    pos_words = 0\n",
    "    neg_words = 0\n",
    "\n",
    "    tokenized_sent = [word.lower() for word in tokenizer.tokenize(row['text'])]\n",
    "\n",
    "    for word in tokenized_sent:\n",
    "        if word in opinion_lexicon.positive():\n",
    "            pos_words += 1\n",
    "        elif word in opinion_lexicon.negative():\n",
    "            neg_words += 1\n",
    "\n",
    "    if pos_words > neg_words:\n",
    "        polarity = 'positive'\n",
    "    elif pos_words < neg_words:\n",
    "        polarity = 'negative'\n",
    "    elif pos_words == neg_words:\n",
    "        polarity = 'neutral'\n",
    "\n",
    "    # print(row['text'] + ': ' + polarity)\n",
    "\n",
    "    return polarity\n",
    "\n",
    "def process_data(func, df, num_processes=None):\n",
    "    if num_processes == None:\n",
    "        num_processes = min(df.shape[0], cpu_count())\n",
    "\n",
    "    with Pool(processes = num_processes) as pool:\n",
    "        seq = []\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            seq.append(row)\n",
    "\n",
    "        results_list = list(tqdm(pool.imap(get_lexicon_polarity, seq), total=len(df.index)))\n",
    "\n",
    "        df['lex_polarity'] = results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = input('Name of .json file: ')\n",
    "\n",
    "infile = name + '.json'\n",
    "outfile = name + '_lex_pol.json'\n",
    "\n",
    "print('reading from ' + infile)\n",
    "\n",
    "corpus = pd.read_json(infile)\n",
    "\n",
    "print('identifying sentiments...')\n",
    "process_data(get_lexicon_polarity, corpus, num_processes=cpu_count())\n",
    "\n",
    "print('saving to ' + outfile)\n",
    "corpus.to_json(path_or_buf=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing sentiment over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_json('james_franco_lex_pol.json')\n",
    "tweets.sort_values(by='timestamp', inplace=True)\n",
    "startDate = date(2017, 12, 15)\n",
    "endDate = date(2018, 2, 15)\n",
    "\n",
    "tweets['timestamp'] = pd.to_datetime(tweets['timestamp']).apply(lambda x: x.date())\n",
    "\n",
    "tweets_stub = tweets[tweets['timestamp'] >= startDate]\n",
    "tweets_stub = tweets_stub[tweets_stub['timestamp'] <= endDate]\n",
    "tweets_stub.sort_values(by='timestamp', inplace=True)\n",
    "\n",
    "df = tweets_stub.groupby(['timestamp', 'lexicon_polarity']).size()\n",
    "df = df.reset_index()\n",
    "df.columns = ['timestamp', 'polarity', 'size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = (20, 7)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "\n",
    "ax = sns.lineplot(ax = ax, x='timestamp', y='size', hue='polarity', palette='pastel', data=df)\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of Tweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regex used to detect words is a combination of normal words, ascii art, and emojis\n",
    "# 2+ consecutive letters (also include apostrophes), e.x It's\n",
    "normal_word = r\"(?:\\w[\\w']+)\"\n",
    "# 2+ consecutive punctuations, e.x. :)\n",
    "ascii_art = r\"(?:[{punctuation}][{punctuation}]+)\".format(punctuation=string.punctuation)\n",
    "# a single character that is not alpha_numeric or other ascii printable\n",
    "emoji = r\"(?:[^\\s])(?<![\\w{ascii_printable}])\".format(ascii_printable=string.printable)\n",
    "regexp = r\"{normal_word}|{ascii_art}|{emoji}\".format(normal_word=normal_word, ascii_art=ascii_art,\n",
    "                                                     emoji=emoji)\n",
    "\n",
    "\n",
    "words = ' '.join(tweets['text'])\n",
    "\n",
    "wordcloud = WordCloud(max_words=500, scale=3, background_color='white', regexp=regexp).generate(no_urls_no_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file('james_franco_cloud.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this website to do word to frequency\n",
    "https://github.com/Mantej-Singh/Word-Frequency---Python/blob/master/Word%20Frequency.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
